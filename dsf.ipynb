{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch scikit-learn nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch pandas nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\siddh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset cleaned! 60 rows available after cleaning.\n",
      "\n",
      "ðŸ¤– Welcome to the Chatbot Decision System!\n",
      "Ask a question below:\n",
      "\n",
      "ðŸ‘¤ You: do you offer free shipping\n",
      "ðŸ¤– standard shipping takes 57 days\n",
      "\n",
      "ðŸ‘¤ You: where can i download my receipt\n",
      "ðŸ¤– invoices can be downloaded from your account\n",
      "\n",
      "ðŸ‘¤ You: can i cancel my order\n",
      "ðŸ¤– to cancel visit your account settings or contact support\n",
      "\n",
      "ðŸ‘¤ You: is there a student discount\n",
      "ðŸ¤– you can upgrade your subscription in the account section\n",
      "\n",
      "ðŸ‘¤ You: when will my product arrive\n",
      "ðŸ¤– your order is on the way check tracking for updates\n",
      "ðŸ‘‹ Exiting Chatbot. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… Step 1: Clean the Dataset Before Processing\n",
    "# =============================================================================\n",
    "def clean_dataset(csv_path):\n",
    "    \"\"\"Cleans the dataset by removing unnecessary columns, duplicates, and missing values.\"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # âœ… Keep only necessary columns\n",
    "    required_columns = [\"category\", \"product_name\", \"product_description\", \"input_text\", \"output_text\"]\n",
    "    df_cleaned = df[required_columns].copy()\n",
    "\n",
    "    # âœ… Remove duplicate input_text entries (keeping the first occurrence)\n",
    "    df_cleaned = df_cleaned.drop_duplicates(subset=[\"input_text\"], keep=\"first\")\n",
    "\n",
    "    # âœ… Remove any rows with missing values in critical columns\n",
    "    df_cleaned = df_cleaned.dropna(subset=[\"input_text\", \"output_text\"])\n",
    "\n",
    "    print(f\"âœ… Dataset cleaned! {df_cleaned.shape[0]} rows available after cleaning.\")\n",
    "    \n",
    "    return df_cleaned\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… Step 2: Decision Tree Node Class\n",
    "# =============================================================================\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, prompt, response=None):\n",
    "        \"\"\"Initialize a decision tree node.\"\"\"\n",
    "        self.prompt = prompt\n",
    "        self.response = response  # Predefined response from dataset\n",
    "        self.children = {}  # Dictionary mapping decisions to child nodes\n",
    "\n",
    "    def add_child(self, decision, node):\n",
    "        \"\"\"Add a child node under the given decision.\"\"\"\n",
    "        self.children[decision.lower()] = node\n",
    "\n",
    "    def traverse(self):\n",
    "        \"\"\"Recursively traverse the decision tree with improved user input handling.\"\"\"\n",
    "        print(\"\\n\" + self.prompt)\n",
    "\n",
    "        while True:\n",
    "            decision = input(\"\\nðŸ‘¤ You: \").strip().lower()  # âœ… Now explicitly displays the userâ€™s question\n",
    "\n",
    "            if decision == \"exit\":\n",
    "                print(\"ðŸ‘‹ Exiting Chatbot. Goodbye!\")\n",
    "                return  # âœ… Ensures chatbot exits cleanly\n",
    "            elif decision == \"back\":\n",
    "                print(\"ðŸ”™ Returning to the main menu...\")\n",
    "                return  # âœ… Allows smooth return to main menu\n",
    "            elif decision in self.children:\n",
    "                print(f\"\\nðŸ‘¤ You: {decision}\")  # âœ… Print user input before response\n",
    "                print(f\"ðŸ¤– {self.children[decision].response}\")  # âœ… Displays bot response\n",
    "            else:\n",
    "                # âœ… Prevent generating a dynamic response if a dataset match exists\n",
    "                if decision in dataset_responses:\n",
    "                    print(f\"\\nðŸ‘¤ You: {decision}\")  # âœ… Print user input before response\n",
    "                    print(f\"ðŸ¤– {dataset_responses[decision]}\")\n",
    "                else:\n",
    "                    print(f\"\\nðŸ‘¤ You: {decision}\")  # âœ… Print user input before response\n",
    "                    print(\"ðŸ¤– I couldn't find an exact match. Let me generate a response for you...\")\n",
    "                    response = model_response_generator(model, tokenizer, decision)\n",
    "                    print(f\"ðŸ¤– {response}\")\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… Step 3: Model Response Generator Function\n",
    "# =============================================================================\n",
    "def model_response_generator(model, tokenizer, user_query):\n",
    "    \"\"\"Generate a response from the fine-tuned model given a user input.\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = tokenizer(user_query, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_new_tokens=64,\n",
    "        pad_token_id=tokenizer.eos_token_id  \n",
    "    )\n",
    "    response = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return response if response else \"ðŸ¤– Sorry, I didn't understand that.\"\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… Step 4: Build Decision Tree from Cleaned CSV\n",
    "# =============================================================================\n",
    "def build_decision_tree_from_csv(df_cleaned):\n",
    "    \"\"\"Build a decision tree from the cleaned dataset CSV using input_text and output_text.\"\"\"\n",
    "    \n",
    "    # âœ… Store all dataset responses for quick lookup\n",
    "    global dataset_responses\n",
    "    dataset_responses = {}\n",
    "\n",
    "    # âœ… Ensure chatbot starts with a structured menu\n",
    "    root_prompt = \"ðŸ¤– Welcome to the Chatbot Decision System!\\nAsk a question below:\"\n",
    "    root = DecisionTreeNode(root_prompt)\n",
    "\n",
    "    for _, row in df_cleaned.iterrows():\n",
    "        input_text, output_text = row[\"input_text\"].strip().lower(), row[\"output_text\"].strip()\n",
    "\n",
    "        # âœ… Ensure chatbot provides dataset responses first\n",
    "        dataset_responses[input_text] = output_text\n",
    "        branch_node = DecisionTreeNode(f\"ðŸ‘¤ {input_text}\", response=output_text)\n",
    "        root.add_child(input_text, branch_node)\n",
    "\n",
    "    return root\n",
    "\n",
    "# =============================================================================\n",
    "# âœ… Step 5: Load Model, Clean Dataset, Build Tree, and Start Chatbot\n",
    "# =============================================================================\n",
    "model_path = r\"C:\\Users\\siddh\\Downloads\\Master Thesis\\Chatbot 2\\fine_tuned_model\"\n",
    "csv_path = r\"C:\\Users\\siddh\\Downloads\\Master Thesis\\Chatbot 2\\chatbot_data.csv\"\n",
    "\n",
    "# âœ… Load tokenizer correctly to avoid errors\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True, use_fast=False)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# âœ… Load the fine-tuned model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, local_files_only=True)\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# âœ… Clean the dataset before building the chatbot\n",
    "df_cleaned = clean_dataset(csv_path)\n",
    "\n",
    "# âœ… Build the decision tree using the cleaned dataset\n",
    "decision_tree = build_decision_tree_from_csv(df_cleaned)\n",
    "\n",
    "# âœ… Start chatbot interaction\n",
    "decision_tree.traverse()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
